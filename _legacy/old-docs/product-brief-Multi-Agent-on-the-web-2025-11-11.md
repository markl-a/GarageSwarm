# Product Brief: Multi-Agent on the Web

**日期：** 2025-11-11
**作者：** sir
**專案類型：** 個人學習項目 + 開源項目
**當前狀態：** 正在創建中...

---

## Executive Summary

**Multi-Agent on the Web** 是一個革命性的多代理協作平台，旨在大幅提升專業開發者使用各類 AI CLI 工具的效率。通過分布式 Worker 架構和智能的多 Agent 協調機制，該平台將單一 AI 工具的線性工作流程轉變為並行、互相審查、人類監督的高效協作模式。

**核心價值主張：**
- ⚡ **速度提升 2-3 倍**：多 Agent 並行執行取代順序執行
- 🎯 **質量保證**：多層次驗證機制（Agent 互審、評估框架、人類把關）
- 👁️ **可視化監控**：實時查看所有 Agent 狀態，一目了然
- 🔧 **靈活糾偏**：隨時介入任何 Agent，防止跑偏
- 🌍 **人人可用**：從專業開發者開始，逐步降低使用門檻

**目標用戶（階段 1）：** 專業開發者，熟悉 Claude Code、Gemini CLI、Codex 等 AI 工具
**演進路徑：** 專業開發者 → 普通開發者 → 更廣泛用戶

---

## Core Vision

### Problem Statement

**當前 AI 輔助開發的痛點：**

1. **效率低下 - 順序執行的瓶頸**

   當前使用 Claude Code、Gemini CLI 等工具時，開發者必須：
   - 完成一個任務後才能開始下一個任務
   - 手動在不同工具間切換
   - 等待每個工具的響應（通常 5-10 分鐘）
   - 反復調整 prompt 才能得到滿意結果

   **具體場景：** 開發一個完整的用戶認證系統
   - 步驟 1：用 Claude Code 生成 API 代碼（10 分鐘）
   - 步驟 2：等待完成，人工檢查
   - 步驟 3：用 Gemini 審查代碼（5 分鐘）
   - 步驟 4：等待完成，人工檢查
   - 步驟 5：用 Codex 寫測試（8 分鐘）
   - 步驟 6：等待完成，人工檢查
   - 步驟 7：用 Local LLM 生成文檔（5 分鐘）
   - **總計：28 分鐘 + 大量人工切換時間**

2. **質量不穩定 - AI 容易"跑偏"**

   AI 工具經常出現：
   - 理解錯誤開發者的需求
   - 生成的代碼風格不一致
   - 忘記之前對話中的上下文
   - 做出不符合項目架構的決策

   **結果：** 開發者必須持續監督，發現問題時需要從頭來過

3. **缺乏協作機制**

   - 不同 AI 工具各自為政，無法協同
   - 沒有自動的質量審查機制
   - 人類必須充當所有工具之間的"協調者"

### Problem Impact

**對專業開發者的影響：**

- **時間浪費：** 大量時間花在等待 AI 響應和工具切換上
- **注意力分散：** 需要持續監督多個工具，無法專注於核心決策
- **效率悖論：** AI 本應提升效率，但實際使用中反而引入了新的瓶頸
- **信任度低：** 擔心 AI 跑偏，不敢放手讓 AI 自動執行

**量化影響：**
- 一個中型功能開發（如用戶認證系統）需要 2-4 小時
- 其中 30-40% 時間浪費在工具切換和等待上
- 20-30% 時間用於發現和修正 AI 的錯誤

### Why Existing Solutions Fall Short

**現有工具的局限性：**

| 工具 | 優勢 | 局限 |
|------|------|------|
| **Claude Code** | 強大的代碼生成能力 | 單一工作流，無法並行；需要人工審查 |
| **Gemini CLI** | 快速響應 | 獨立運行，無法與其他工具協作 |
| **GitHub Copilot** | 實時代碼補全 | 被動工具，不能主動執行複雜任務 |
| **AutoGPT/AgentGPT** | 嘗試自動化 | 容易跑偏，缺乏人類控制；質量不穩定 |
| **LangChain/CrewAI** | 提供框架 | 需要大量編程，不是"開箱即用"的產品 |

**核心缺陷：**
1. ❌ 沒有產品化的多 Agent 可視化協作平台
2. ❌ 沒有靈活的"半自動"機制（人類在關鍵點把關）
3. ❌ 沒有跨工具的統一監控和糾偏界面
4. ❌ 沒有內建的質量評估框架

### Proposed Solution

**Multi-Agent on the Web：半自動的人類助理平台**

核心概念：通過**可視化的多 Agent 協作平台**，將線性的 AI 工作流程轉變為並行、互相審查、人類監督的高效協作模式。

**解決方案架構：**

```
Flutter 跨平台客戶端（統一控制中心）
    ↓
可視化儀表板
  ├─ 總體狀況一目了然
  ├─ 每個 Agent 的實時狀態
  ├─ 點擊進入 Agent 詳情
  └─ 糾偏和建議介面
    ↓
後端協調服務器
  ├─ 智能任務分解
  ├─ Agent 並行調度
  ├─ 評估框架引擎
  └─ 人類確認檢查點
    ↓
分布式 Worker 節點
  ├─ Machine-1: Claude Code + Local LLM
  ├─ Machine-2: Gemini CLI + Codex
  └─ Machine-N: 可擴展
```

**核心功能：**

1. **並行加速（2-3 倍速度提升）**
   - 自動將大任務分解為子任務
   - 多個 Agent 並行執行
   - 智能依賴管理（某些任務必須等待前置任務完成）

2. **多層質量保證機制**

   **機制 A：Agent 互相審查**
   - Agent 1 生成代碼 → Agent 2 自動審查 → 發現問題 → Agent 1 修復

   **機制 B：人類關鍵節點把關**
   - 在重要決策點自動暫停
   - 用戶審查並確認或糾偏
   - 靈活設置檢查點頻率

   **機制 C：多方案投票**
   - 對於關鍵任務，3 個 Agent 並行生成方案
   - 展示所有方案供用戶選擇
   - 或使用評估框架自動選擇最佳方案

   **機制 D：評估框架（創新）**
   - 內建多維度評估標準
   - 自動評估 Agent 輸出質量
   - 提供量化評分和改進建議

3. **實時可視化監控**
   - 類似任務管理器的界面
   - 實時顯示每個 Agent 的工作狀態
   - 點擊進入任意 Agent 查看詳細輸出
   - 支持實時介入和糾偏

4. **靈活的人機協作模式**
   - 全自動模式（信任度高的任務）
   - 半自動模式（關鍵點人工確認）
   - 手動模式（完全由人類控制）

### Key Differentiators

**與現有解決方案的根本區別：**

| 維度 | Multi-Agent on the Web | 現有工具 |
|------|------------------------|----------|
| **協作模式** | ✅ 多 Agent 並行 + 互相審查 | ❌ 單一 Agent 順序執行 |
| **可視化** | ✅ 實時儀表板，一目了然 | ❌ 命令行輸出，難以追蹤 |
| **質量保證** | ✅ 4 層機制（互審、人工、投票、評估） | ❌ 依賴人工檢查 |
| **糾偏能力** | ✅ 隨時介入任何 Agent | ❌ 發現問題需重新開始 |
| **分布式** | ✅ 利用多台機器資源 | ❌ 單機運行 |
| **隱私保護** | ✅ 敏感任務使用本地 LLM | ❌ 全部依賴雲端 API |
| **開箱即用** | ✅ 可視化產品，無需編程 | ❌ 需要編寫代碼配置 |

**獨特價值：**

1. **"半自動"的完美平衡**
   - 不是完全自動化（容易失控）
   - 也不是簡單工具（效率低）
   - 而是 AI 執行 + 人類把關的理想結合

2. **評估框架（創新點）**
   - 內建可擴展的質量評估標準
   - 自動量化 Agent 輸出質量
   - 持續學習和改進

3. **真正的分布式協作**
   - 利用用戶多台機器的計算資源
   - 本地 LLM + 雲端 API 混合使用
   - 橫向擴展，性能無上限

---

## Target Users

### Primary Users (階段 1)

**專業開發者（Professional Developers）**

**用戶畫像：**
- 經驗豐富的軟體工程師（5+ 年經驗）
- 已經在使用 AI 輔助工具（Claude Code、Copilot、Gemini 等）
- 理解代碼架構和設計模式
- 熟悉命令行工具
- 願意嘗試新技術

**具體場景：**

**場景 1：全棧開發者 Alex**
- 需要快速開發 MVP，時間緊迫
- 使用 Claude Code 生成後端，Copilot 輔助前端
- 痛點：來回切換工具，等待時間長
- 期望：一次性提交任務，多個 Agent 並行完成，他只需要在關鍵點審查

**場景 2：架構師 Sarah**
- 負責設計複雜系統，需要 AI 輔助研究和方案對比
- 使用 Gemini 研究技術方案，Claude 生成架構文檔
- 痛點：擔心 AI 生成的方案不夠全面或有偏差
- 期望：多個 Agent 提供不同方案，她可以對比選擇最佳方案

**場景 3：開源維護者 Mike**
- 維護多個開源項目，需要快速響應 PR 和 Issue
- 使用 AI 工具審查代碼、生成文檔、回復問題
- 痛點：每個任務都需要手動啟動 AI，效率低
- 期望：自動化流程，AI 並行處理多個任務，他只需要最後確認

**用戶需求優先級：**

1. 🔴 **必須有：** 實時可視化監控（看清所有 Agent 在做什麼）
2. 🔴 **必須有：** 靈活糾偏能力（隨時介入）
3. 🔴 **必須有：** 多 Agent 並行執行（速度提升）
4. 🟡 **重要：** 質量評估框架（自動評估輸出質量）
5. 🟡 **重要：** 本地 LLM 支持（保護隱私）
6. 🟢 **有更好：** 預設工作流模板

**技術能力：**
- ✅ 能理解分布式系統概念
- ✅ 能配置 Worker Agent
- ✅ 能寫有效的 prompt
- ✅ 能理解架構設計

### Secondary Users (階段 2 - 未來)

**普通開發者（Regular Developers）**

預計在項目成熟後（6-12 個月）開始支持：

**用戶特徵：**
- 1-5 年經驗的開發者
- 第一次使用 AI 輔助工具
- 對架構設計不太熟悉
- 需要更多引導和模板

**需要額外提供：**
- 預設的工作流模板（"開發 REST API"、"添加用戶認證"）
- AI 助手幫助撰寫 prompt
- 簡化的界面（隱藏高級選項）
- 更詳細的教學和文檔

### User Journey (專業開發者)

**典型使用流程：**

```
第 1 步：啟動平台
  ├─ 打開 Flutter 應用（Desktop/Web）
  └─ 看到儀表板：顯示已連接的 Worker 機器

第 2 步：提交任務
  ├─ 點擊 "新建任務"
  ├─ 選擇任務類型："開發完整功能"
  ├─ 描述需求："開發用戶認證系統（JWT）"
  └─ 提交

第 3 步：自動分解和分配
  ├─ 後端自動分解為子任務：
  │   ├─ 子任務 1：生成認證 API
  │   ├─ 子任務 2：生成測試
  │   └─ 子任務 3：生成文檔
  ├─ 分配給不同 Agent 並行執行
  └─ 用戶在儀表板看到實時進度

第 4 步：並行執行 + 實時監控
  ├─ Agent 1 (Claude): 生成認證 API [進行中] ⏳
  ├─ Agent 2 (Codex): 準備測試框架 [完成] ✓
  └─ Agent 3 (Local LLM): 準備文檔模板 [完成] ✓

第 5 步：第一個檢查點 - 代碼審查
  ├─ Agent 1 完成代碼生成
  ├─ 自動觸發 Agent 4 (Gemini) 進行審查
  ├─ Agent 4 發現問題："建議使用 bcrypt 加密密碼"
  └─ 系統暫停，等待用戶確認 🛑

第 6 步：用戶審查和決策
  ├─ 用戶點擊進入 Agent 1 詳情
  ├─ 查看生成的代碼
  ├─ 查看 Agent 4 的審查報告
  ├─ 決策：
  │   ├─ 選項 A：接受建議，讓 Agent 1 修復
  │   ├─ 選項 B：忽略建議
  │   └─ 選項 C：手動修改部分代碼
  └─ 選擇 A，繼續 ▶️

第 7 步：修復和驗證
  ├─ Agent 1 根據建議修復代碼
  ├─ Agent 4 重新審查 → 通過 ✓
  └─ 自動進入下一階段

第 8 步：測試和文檔生成
  ├─ Agent 2 (Codex): 基於代碼生成測試 [並行]
  └─ Agent 3 (Local LLM): 生成 API 文檔 [並行]

第 9 步：最終審查
  ├─ 所有 Agent 完成
  ├─ 系統展示完整結果：
  │   ├─ 認證 API 代碼
  │   ├─ 測試套件
  │   └─ API 文檔
  ├─ 評估框架自動評分：
  │   ├─ 代碼質量：8.5/10
  │   ├─ 測試覆蓋率：85%
  │   └─ 文檔完整性：9/10
  └─ 用戶最終審查並確認 ✓

第 10 步：完成
  └─ 總耗時：12 分鐘（vs 傳統方式 28 分鐘）
```

---

## Success Metrics

### 階段 1 成功指標（專業開發者採用）

**用戶採用指標：**
- 🎯 **目標：** 6 個月內 100 個活躍用戶
- 🎯 **目標：** 平均每個用戶每週使用 5 次以上
- 🎯 **目標：** 用戶留存率 > 60%（30 天）

**效率提升指標：**
- 🎯 **目標：** 任務完成時間平均減少 40-50%
- 🎯 **目標：** 用戶報告的"時間節省"滿意度 > 8/10
- 🎯 **目標：** Agent 並行執行率 > 70%

**質量指標：**
- 🎯 **目標：** Agent 輸出質量評分平均 > 7.5/10
- 🎯 **目標：** 需要糾偏的任務比例 < 30%
- 🎯 **目標：** 用戶對質量滿意度 > 7.5/10

**技術指標：**
- 🎯 **目標：** 平台穩定性 > 99%（Worker 在線率）
- 🎯 **目標：** 平均任務響應時間 < 2 秒
- 🎯 **目標：** 支持至少 3 種主流 AI 工具整合

### 階段 2 成功指標（開源社區成長）

**開源社區：**
- 🎯 **目標：** GitHub Stars > 1000
- 🎯 **目標：** 活躍貢獻者 > 20 人
- 🎯 **目標：** 社區提交的 PR > 50

**平台擴展：**
- 🎯 **目標：** 支持 5+ 種 AI 工具
- 🎯 **目標：** 用戶自定義工作流模板 > 50 個
- 🎯 **目標：** 降低使用門檻，支持普通開發者

---

## MVP Scope

### Core Features (MVP 必須有)

**1. 可視化儀表板 (Flutter 跨平台)**
   - 總體狀況顯示（多少機器在線、多少任務運行中）
   - 機器列表（顯示每台機器的狀態、工具、資源使用）
   - 任務列表（當前任務、歷史任務）
   - 實時日誌流

**2. 分布式 Worker Agent 系統**
   - Worker 自動註冊和心跳
   - 支持 3 種 AI 工具：
     - Claude Code (MCP)
     - Gemini CLI
     - Local LLM (Ollama)
   - 資源監控（CPU、內存、磁盤）
   - 任務執行和結果上報

**3. 智能任務協調（後端）**
   - 任務分解（將大任務拆解為子任務）
   - 智能分配（根據工具、資源、隱私需求分配）
   - 並行調度（支持多個 Agent 並行執行）
   - 依賴管理（某些任務必須等待前置任務）

**4. 人類確認檢查點**
   - 關鍵點自動暫停
   - 展示 Agent 輸出
   - 用戶確認/糾偏/拒絕介面
   - 糾偏後繼續執行

**5. Agent 互相審查機制**
   - 代碼生成完成後自動觸發審查 Agent
   - 審查報告展示
   - 自動修復循環

**6. 基礎評估框架**
   - 簡單的質量評分（代碼質量、完整性）
   - 評分結果展示
   - 低於閾值自動告警

### Out of Scope for MVP

**暫不包含的功能（留給未來版本）：**

❌ **多方案投票機制**
   - 複雜度高，MVP 先專注於單方案 + 審查
   - 可以在 v1.1 或 v1.2 添加

❌ **預設工作流模板**
   - MVP 支持自由任務描述
   - 模板系統需要大量測試和優化

❌ **高級評估框架**
   - MVP 只提供基礎評分
   - 機器學習驅動的評估留給後期

❌ **移動端完整支持**
   - MVP 專注於桌面端（Flutter Desktop）和 Web
   - 移動端只提供基本查看功能

❌ **用戶認證和多用戶**
   - MVP 是單用戶本地運行
   - 雲端部署和多用戶留給後期

❌ **Codex API 整合**
   - MVP 先專注於 Claude 和 Gemini
   - Codex 整合可以延後

❌ **高級可視化（圖表、趨勢分析）**
   - MVP 只顯示實時狀態
   - 歷史數據分析留給後期

### MVP Success Criteria

**MVP 成功的標準：**

✅ **功能完整性：**
   - [ ] 能成功註冊和管理至少 2 台 Worker 機器
   - [ ] 能並行執行至少 2 個 Agent 任務
   - [ ] 能實時顯示 Agent 狀態和輸出
   - [ ] 能在檢查點暫停並接受用戶糾偏
   - [ ] 能觸發 Agent 互相審查

✅ **性能指標：**
   - [ ] 任務完成時間比順序執行快 30% 以上
   - [ ] Worker 註冊和心跳延遲 < 1 秒
   - [ ] 前端實時更新延遲 < 500ms

✅ **穩定性：**
   - [ ] 連續運行 4 小時無崩潰
   - [ ] Worker 離線後能自動重連
   - [ ] 任務失敗能自動重試或報錯

✅ **用戶體驗：**
   - [ ] 初次使用者能在 10 分鐘內完成設置
   - [ ] 界面清晰，能一目了然看到系統狀態
   - [ ] 糾偏操作直觀，不需要查看文檔

✅ **質量驗證：**
   - [ ] 基礎評估框架能給出合理評分
   - [ ] Agent 審查能發現明顯問題（如代碼錯誤、風格問題）

### Future Vision Features

**下一階段計劃（v1.1 - v1.2）：**

🔮 **多方案投票機制**
   - 對於關鍵任務，3 個 Agent 並行生成方案
   - 用戶可以對比選擇
   - 或使用評估框架自動選擇最佳方案

🔮 **預設工作流模板**
   - "開發 REST API"
   - "添加用戶認證"
   - "實現支付集成"
   - "代碼重構"
   - 用戶可以自定義和分享模板

🔮 **高級評估框架**
   - 基於機器學習的質量預測
   - 代碼安全性掃描
   - 性能評估
   - 可擴展的自定義評估標準

🔮 **移動端完整體驗**
   - 移動端也能提交任務
   - 移動端實時監控
   - 移動端糾偏

🔮 **雲端部署選項**
   - 除了本地運行，提供雲端部署
   - 多用戶支持
   - 團隊協作功能

🔮 **更多 AI 工具整合**
   - Codex (OpenAI)
   - GitHub Copilot API
   - Anthropic Claude API
   - Google Gemini Pro
   - 開放插件系統，用戶可自定義工具

🔮 **智能學習和優化**
   - 學習用戶的糾偏習慣
   - 自動調整檢查點頻率
   - 智能推薦最佳 Agent 組合

---

## Evaluation Framework (創新核心)

### Framework Overview

**評估框架（Evaluation Framework）** 是本平台的核心創新之一，旨在通過**量化、可擴展、自動化**的方式評估 Agent 輸出質量，從而實現：

1. **自動質量把關**：在人類審查前先進行機器評估，過濾明顯問題
2. **持續改進**：量化評分幫助追蹤質量趨勢
3. **智能決策**：在多方案投票中自動選擇最佳方案

### Core Evaluation Dimensions

評估框架包含 **5 個核心維度**，每個維度可獨立評分（0-10 分）：

#### 1. Code Quality（代碼質量）

**評估標準：**
- **語法正確性**：代碼能否正常編譯/運行
- **代碼風格**：是否符合項目的 linting 規則
- **可讀性**：命名是否清晰、註釋是否充足
- **復雜度**：圈複雜度是否合理（避免超長函數）
- **最佳實踐**：是否遵循語言和框架的最佳實踐

**MVP 實現方式：**
```
靜態分析工具整合：
├─ Python: pylint, black, mypy
├─ JavaScript: ESLint, Prettier
├─ TypeScript: TSLint
└─ 通用: SonarQube API
```

**評分邏輯示例：**
```
基礎分 = 10
- 語法錯誤：-5 分
- Linting 警告：每個 -0.5 分（最多扣 3 分）
- 圈複雜度 > 15：每個函數 -1 分
- 缺少註釋（公開 API）：-1 分

最終分數 = max(基礎分 - 扣分, 0)
```

#### 2. Completeness（完整性）

**評估標準：**
- **需求覆蓋**：是否實現了所有要求的功能
- **邊界情況處理**：是否考慮錯誤處理、空值處理
- **測試覆蓋**：是否包含測試代碼
- **文檔完整**：是否包含必要的文檔

**MVP 實現方式：**
```
基於檢查清單（Checklist）：
├─ 解析用戶原始需求
├─ 提取關鍵功能點
├─ 檢查 Agent 輸出是否覆蓋每個功能點
└─ 計算覆蓋率
```

**評分邏輯示例：**
```
功能點完成率 × 8 分 + 額外加分項

額外加分項：
+ 錯誤處理：+1 分
+ 單元測試：+1 分
+ 文檔/註釋：+1 分

最大分數 = 10
```

#### 3. Security（安全性）

**評估標準：**
- **注入攻擊防護**：SQL injection、XSS、Command injection
- **認證授權**：是否正確實現認證和授權檢查
- **數據驗證**：輸入是否經過驗證
- **敏感數據處理**：密碼、API Key 是否加密
- **依賴項漏洞**：第三方庫是否有已知漏洞

**MVP 實現方式：**
```
安全掃描工具整合：
├─ 靜態分析: Bandit (Python), ESLint Security
├─ 依賴掃描: npm audit, pip-audit
├─ 正則表達式檢查: 檢測常見安全反模式
└─ 敏感數據洩露檢測: 檢查硬編碼密碼、API Key
```

**評分邏輯示例：**
```
基礎分 = 10
- 高危漏洞：每個 -3 分
- 中危漏洞：每個 -1 分
- 低危漏洞：每個 -0.5 分
- 硬編碼敏感信息：-2 分

最終分數 = max(基礎分 - 扣分, 0)

⚠️ 如果分數 < 4，自動標記為 "需要人工審查"
```

#### 4. Architecture Alignment（架構一致性）

**評估標準：**
- **設計模式**：是否遵循項目的設計模式
- **依賴關係**：是否引入不必要的依賴
- **分層架構**：是否符合項目的分層結構
- **API 設計**：API 設計是否與現有風格一致

**MVP 實現方式：**
```
上下文感知評估：
├─ 讀取項目架構文檔（如果有）
├─ 分析現有代碼庫的模式
├─ 檢查 Agent 生成的代碼是否符合
└─ 使用 LLM 進行語義分析
```

**評分邏輯示例（基於 LLM）：**
```
使用本地 LLM 或輕量級模型（Gemini Flash）：

Prompt:
"分析以下代碼是否符合項目架構：
項目架構: {從文檔中提取}
現有代碼模式: {從代碼庫中提取}
新生成代碼: {Agent 輸出}

評分維度（0-10）：
1. 是否遵循相同的設計模式
2. 是否使用項目已有的依賴
3. 是否符合分層架構
4. API 設計是否一致

輸出 JSON:
{
  "score": 8,
  "reasons": ["...", "..."],
  "suggestions": ["..."]
}
"
```

#### 5. Testability（可測試性）

**評估標準：**
- **單元測試**：是否包含單元測試
- **測試覆蓋率**：測試覆蓋關鍵邏輯的比例
- **測試質量**：測試是否有意義（非空測試）
- **Mock 能力**：代碼是否易於 mock 和測試

**MVP 實現方式：**
```
測試分析：
├─ 檢測是否有測試文件
├─ 運行測試並計算覆蓋率（如果可能）
├─ 分析測試的質量（是否有斷言）
└─ 檢查依賴注入等可測試性模式
```

**評分邏輯示例：**
```
有測試文件：5 分（基礎分）

+ 測試覆蓋率 > 80%：+3 分
+ 測試覆蓋率 60-80%：+2 分
+ 測試覆蓋率 < 60%：+1 分

+ 測試有意義（包含斷言）：+1 分
+ 使用依賴注入等模式：+1 分

最大分數 = 10
```

### Aggregated Scoring

**總體評分計算：**

```
總分 = 加權平均分

權重配置（可自定義）：
├─ Code Quality: 25%
├─ Completeness: 30%
├─ Security: 25%
├─ Architecture Alignment: 10%
└─ Testability: 10%

總分 = 0.25×代碼質量 + 0.30×完整性 + 0.25×安全性
       + 0.10×架構一致性 + 0.10×可測試性

質量等級：
├─ 9-10 分: Excellent（優秀）✅
├─ 7-8.9 分: Good（良好）✅
├─ 5-6.9 分: Acceptable（可接受）⚠️
├─ 3-4.9 分: Poor（較差）❌ → 需要人工審查
└─ 0-2.9 分: Fail（失敗）❌ → 必須重做
```

### Framework Implementation

**實現架構：**

```
評估框架服務（Backend）
├─ 評估引擎 (Python)
│   ├─ dimension_evaluators/
│   │   ├─ code_quality.py
│   │   ├─ completeness.py
│   │   ├─ security.py
│   │   ├─ architecture.py
│   │   └─ testability.py
│   ├─ aggregator.py（加權計算）
│   └─ report_generator.py（生成報告）
│
├─ 工具整合
│   ├─ static_analysis_adapter.py
│   ├─ security_scanner_adapter.py
│   └─ llm_evaluator_adapter.py
│
└─ 配置管理
    └─ evaluation_config.yaml（權重、閾值）
```

**評估流程：**

```
Agent 完成任務
  ↓
後端自動觸發評估框架
  ↓
並行執行 5 個維度的評估
  ├─ Code Quality Evaluator → 8.5/10
  ├─ Completeness Evaluator → 9.0/10
  ├─ Security Evaluator → 7.0/10 ⚠️
  ├─ Architecture Evaluator → 8.0/10
  └─ Testability Evaluator → 6.5/10
  ↓
聚合評分 → 7.8/10 (Good)
  ↓
生成評估報告
  ↓
展示給用戶
  ├─ 如果 < 5 分 → 自動標記"需要重做"
  ├─ 如果 5-7 分 → 標記"需要人工審查"
  └─ 如果 > 7 分 → 可以繼續
```

### MVP Implementation Priority

**MVP 階段實現的評估維度：**

✅ **Phase 1 (MVP):**
- Code Quality（代碼質量）- 優先級最高
- Completeness（完整性）- 優先級高
- Security（安全性）- 優先級高（基礎掃描）

⏳ **Phase 2 (Post-MVP):**
- Architecture Alignment（架構一致性）- 需要更複雜的上下文理解
- Testability（可測試性）- 需要測試執行環境

🔮 **Future:**
- Performance Evaluation（性能評估）
- User Experience Evaluation（用戶體驗評估）
- Accessibility Evaluation（無障礙評估）

### User Experience

**用戶如何與評估框架交互：**

**場景 1：自動評估通過**
```
Agent 完成任務 → 評估框架自動評分 → 8.2/10 (Good)
  ↓
用戶收到通知: "任務完成，質量評分：8.2/10 ✅"
  ↓
用戶查看詳細報告:
  ├─ 代碼質量：8.5/10 ✅
  ├─ 完整性：9.0/10 ✅
  ├─ 安全性：7.0/10 ⚠️
  │   └─ 建議：添加輸入驗證
  └─ 總體：Good，可以繼續
  ↓
用戶選擇:
  ├─ 接受並繼續
  └─ 或查看詳細代碼手動審查
```

**場景 2：評估發現問題**
```
Agent 完成任務 → 評估框架自動評分 → 4.5/10 (Poor) ❌
  ↓
用戶收到警告: "質量評分較低，需要人工審查"
  ↓
用戶查看詳細報告:
  ├─ 代碼質量：3.0/10 ❌
  │   └─ 問題：發現 5 個語法錯誤
  ├─ 完整性：6.0/10 ⚠️
  │   └─ 問題：缺少錯誤處理
  └─ 安全性：4.0/10 ❌
      └─ 問題：發現 SQL injection 風險
  ↓
系統建議:
  ├─ 選項 A：讓 Agent 根據報告自動修復
  ├─ 選項 B：用戶手動修改
  └─ 選項 C：重新生成
```

### Extensibility

**可擴展性設計：**

評估框架設計為**可插拔架構**，用戶和社區可以：

1. **自定義評估維度**
   ```yaml
   # custom_evaluation.yaml
   dimensions:
     - name: "Performance"
       weight: 0.15
       evaluator: "custom_evaluators.performance"
       config:
         max_response_time_ms: 200
   ```

2. **調整權重和閾值**
   ```yaml
   # evaluation_config.yaml
   weights:
     code_quality: 0.30  # 調整為 30%
     security: 0.40      # 提高安全權重

   thresholds:
     fail: 4.0    # 低於 4 分視為失敗
     warning: 7.0 # 低於 7 分顯示警告
   ```

3. **編寫自定義評估器**
   ```python
   # custom_evaluators/performance.py
   class PerformanceEvaluator(BaseEvaluator):
       def evaluate(self, code, context):
           # 自定義評估邏輯
           score = self._measure_performance(code)
           return {
               "score": score,
               "details": {...}
           }
   ```

---

## Technical Preferences

### Technology Stack

**前端（Flutter 跨平台）：**
- Framework: Flutter 3.x+
- State Management: Riverpod 或 Bloc
- WebSocket: web_socket_channel
- UI: Material Design 3

**後端（Python FastAPI）：**
- Framework: FastAPI 0.100+
- Database: PostgreSQL 15+ (持久化)
- Cache: Redis 7+ (實時狀態)
- Message Queue: Redis Pub/Sub 或 RabbitMQ
- ORM: SQLAlchemy 2.x

**Worker Agent（Python）：**
- 語言: Python 3.11+
- AI 工具整合:
  - Claude Code: MCP (Model Context Protocol)
  - Gemini: Google AI SDK
  - Local LLM: Ollama API
- 異步: asyncio
- 配置: YAML

**評估框架：**
- 靜態分析: pylint, ESLint, SonarQube
- 安全掃描: Bandit, npm audit
- LLM 評估: Gemini Flash（輕量級）或本地 LLM

**DevOps：**
- 容器化: Docker + Docker Compose
- CI/CD: GitHub Actions
- 監控: Prometheus + Grafana（可選）

### Architecture Constraints

**設計約束：**

1. **本地優先**
   - MVP 專注於本地部署
   - 所有數據存儲在用戶本地
   - 不依賴雲端服務（除了 AI API）

2. **跨平台**
   - Frontend 必須支持 Desktop（Windows/Mac/Linux）和 Web
   - Worker Agent 必須支持主流操作系統

3. **可擴展性**
   - 架構設計支持未來添加更多 AI 工具
   - 評估框架必須可插拔
   - 工作流系統必須可配置

4. **隱私保護**
   - 敏感任務使用本地 LLM
   - 用戶可以選擇哪些任務發送到雲端
   - 不收集用戶數據（開源項目）

---

## Risks and Assumptions

### Key Assumptions

**技術假設：**
1. ✅ Claude Code 的 MCP 協議穩定可用
2. ✅ Gemini CLI 和 Ollama 提供可靠的 API
3. ✅ WebSocket 能滿足實時通信需求
4. ⚠️ 用戶有多台機器可以部署 Worker（或願意在單機運行多個 Worker）
5. ⚠️ 用戶熟悉命令行工具，能配置 Worker Agent

**用戶假設：**
1. ✅ 專業開發者願意嘗試新工具
2. ✅ 用戶能理解"半自動"的概念
3. ⚠️ 用戶願意花時間學習平台（學習曲線）
4. ⚠️ 用戶有足夠的計算資源運行本地 LLM

**市場假設：**
1. ✅ AI 輔助開發是持續增長的趨勢
2. ⚠️ 多 Agent 協作能真正提升效率（需驗證）
3. ⚠️ 開源社區會貢獻和支持項目

### Major Risks

**技術風險：**

| 風險 | 影響 | 可能性 | 緩解策略 |
|------|------|--------|----------|
| **AI 工具 API 變更** | 高 | 中 | 設計適配器層，隔離 API 變化 |
| **實時通信延遲** | 中 | 低 | 使用 WebSocket + 本地緩存 |
| **Worker 穩定性問題** | 高 | 中 | 實現心跳、自動重連、任務重試 |
| **評估框架準確性** | 中 | 高 | MVP 先使用簡單規則，逐步優化 |
| **跨平台兼容性** | 中 | 中 | 選擇成熟的跨平台框架（Flutter） |

**產品風險：**

| 風險 | 影響 | 可能性 | 緩解策略 |
|------|------|--------|----------|
| **學習曲線過高** | 高 | 中 | 提供詳細文檔和視頻教學 |
| **用戶不願分布式部署** | 中 | 中 | 支持單機多 Worker 模式 |
| **質量不如預期** | 高 | 中 | 強化評估框架，提供更多檢查點 |
| **速度提升不明顯** | 高 | 低 | 優化任務分解和調度算法 |

**商業風險（開源項目）：**

| 風險 | 影響 | 可能性 | 緩解策略 |
|------|------|--------|----------|
| **社區參與度低** | 中 | 中 | 提供清晰的貢獻指南，積極互動 |
| **競爭對手出現** | 低 | 中 | 專注於差異化（半自動、評估框架） |
| **維護負擔過重** | 中 | 中 | 建立核心貢獻者團隊 |

---

## Open Questions

**需要進一步驗證的問題：**

1. **任務分解算法**
   - 如何智能地將大任務分解為子任務？
   - 是否需要 LLM 輔助分解？
   - 如何處理任務之間的依賴關係？

2. **檢查點頻率**
   - 默認檢查點頻率應該是多少？
   - 用戶如何自定義檢查點規則？
   - 是否需要"學習模式"自動調整頻率？

3. **評估框架權重**
   - 5 個評估維度的默認權重是否合理？
   - 不同類型任務（API 開發 vs 前端開發）是否需要不同權重？

4. **本地 LLM 性能**
   - 本地 LLM 的評估質量能否滿足需求？
   - 是否需要混合使用（簡單評估用本地，複雜評估用雲端）？

5. **多方案投票機制（Post-MVP）**
   - 何時觸發多方案投票？
   - 如何展示多個方案供用戶選擇？
   - 評估框架如何自動選擇最佳方案？

---

## Next Steps

### Phase 0: 完成規劃（當前）

✅ **已完成：**
- [x] 腦暴會議（Brainstorming Session）
- [x] Product Brief（本文檔）

🔄 **進行中：**
- [ ] 完成 Product Brief 並獲得確認

⏳ **待辦：**
- [ ] 創建 PRD (Product Requirements Document)
- [ ] 設計 UX/UI（可選，推薦）
- [ ] 架構設計（Architecture Document）

### Phase 1: MVP 開發

**預計時間：10-15 週**

**Week 1-3：核心基礎設施**
- [ ] 設計和實現數據模型（PostgreSQL + Redis）
- [ ] 實現後端 API 框架（FastAPI）
- [ ] 實現 Worker Agent 註冊和心跳
- [ ] 實現基礎任務隊列

**Week 4-6：Flutter 前端**
- [ ] 實現儀表板主界面
- [ ] 實現機器列表和狀態顯示
- [ ] 實現任務列表
- [ ] 實現實時 WebSocket 連接

**Week 7-10：AI 工具整合**
- [ ] 整合 Claude Code (MCP)
- [ ] 整合 Gemini CLI
- [ ] 整合 Ollama (Local LLM)
- [ ] 實現任務執行和結果上報

**Week 11-13：智能協調**
- [ ] 實現任務分解邏輯
- [ ] 實現智能任務分配
- [ ] 實現並行調度
- [ ] 實現 Agent 互審機制

**Week 14-15：評估框架和檢查點**
- [ ] 實現基礎評估框架（Code Quality + Completeness + Security）
- [ ] 實現人類確認檢查點
- [ ] 實現糾偏介面
- [ ] 測試和優化

### Phase 2: 測試和優化

**預計時間：3-5 週**

- [ ] 端到端測試
- [ ] 性能測試和優化
- [ ] 用戶體驗優化
- [ ] 文檔撰寫
- [ ] 部署指南

### Phase 3: 發布和收集反饋

**預計時間：持續**

- [ ] 開源發布（GitHub）
- [ ] 撰寫博客文章
- [ ] 製作演示視頻
- [ ] 收集用戶反饋
- [ ] 迭代改進

---

## Document Status

**狀態：** ✅ 已完成

**創建日期：** 2025-11-11
**最後更新：** 2025-11-11
**作者：** sir
**審查者：** 待審查

**版本歷史：**
- v1.0 (2025-11-11): 初始版本，完整 Product Brief

**下一步：**
1. 用戶審查和確認 Product Brief
2. 更新 workflow status，標記 product-brief 為完成
3. 進入 Phase 1: Planning - 創建 PRD (Product Requirements Document)
